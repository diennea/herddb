# standalone|cluster
server.mode=standalone

# listening endpoint for client and server-to-server communications
# if you leave host empty a discovery of the local name will be attempted
server.host=
# if you leave port to zero at every boot a random port will be used
server.port=7000

# id of the node in the cluster
# when empty the system will choose a random id and write it to server.data.dir/nodeid file
server.node.id=

# main directory, other directories by default are relative to this directory
server.base.dir=dbdata

# directory to use to store data
server.data.dir=data

# temporary directory for swap
server.tmp.dir=tmp

# metadata directory. in clustered mode metadata are store on ZooKeeper, not in this directory
server.metadata.dir=metadata

#####################################
# standalone mode txlog configuration
#####################################

# this directory contains the transaction log
server.log.dir=txlog

# max number of unflushed operation
# txlog.maxsyncbatchsize=10000
# max number of unflushed bytes
# txlog.maxsyncbatchbytes=524288
# max time (in ms) to wait before acknowledging a write
# txlog.synctimeout=1
# background fsync task period, in seconds, 0 to disable
# txlog.deferredsyncperiod=0
# max txlog file size
# txlog.maxfilesize=67108864;

# force fsync on txlog (only standalone) and on data pages (standalone and cluster)
#requirefsync=true

# use O_DIRECT to write to the txlog. It may spped up writes up to 50x but
# it depends on the underlying FS and Linux kernel version and configuration
# txlog.use_o_direct=false

# use O_DIRECT to read/write data pages
# page.use_o_direct=false

# use O_DIRECT to read/write index pages
# index.use_o_direct=false

# SSL configuration
# if no file is configured a self signed certificate will be generated at every boot
server.ssl=false

# Enable/Disable network acceptor
# server.network.enabled=true

# for server.mode=cluster
server.zookeeper.address=localhost:2181
server.zookeeper.session.timeout=40000
server.zookeeper.path=/herd


# bookkeeper client parameters
server.bookkeeper.ensemble.size=1
server.bookkeeper.write.quorum.size=1
server.bookkeeper.ack.quorum.size=1

# bookkeeper client parameters. for a fault-tolerant system use should have at least 3 bookies and configure these values
#server.bookkeeper.ensemble.size=3
#server.bookkeeper.write.quorum.size=2
#server.bookkeeper.ack.quorum.size=2

# retention period, in milliseconds, of bookkeeper ledgers
server.bookkeeper.ledgers.retention.period=34560000

# max time to wait before forcing sync to follower nodes, set 0 to disable this feature (if you do not have followers at all)
server.bookkeeper.max.idle.time=10000

# start a bookie inside the same JVM (if the server is started in cluster mode)
server.bookkeeper.start=true
# if you leave port to zero a random port will be used an then persisted to bookie_port file
# bookkeeper uses local hostname and this port to identify bookies
server.bookkeeper.port=-1

# max "logical" size in bytes of a data page. Defaults to 1MB
#server.memory.page.size=

# period of time in milliseconds between forced checkpoints. Defaults to 15 minutes
#server.checkpoint.period=

# Maximum target time in milliseconds to spend during standard checkpoint operations. Checkpoint duration
# could be longer than this to complete pages flush. If set to -1 checkpoints won't have a time limit. Be
# aware that configuring this parameter to small values could impact performances on the long run
# increasing pages pollution with dirty not reclaimed records: in many cases is safer to configure a
# wider dirty page threshold.
#server.checkpoint.duration=

# Maximum target time in milliseconds to spend during standard checkpoint operations on clening dirty
# pages. Is should be less than the maximum checkpoint duration configured by
# # "server.checkpoint.duration". If set to -1 checkpoints won't have a time limit. Regardless his
# value at least one page will be cleaned for each checkpoint.
#server.checkpoint.cleanup=

# Maximum target time in milliseconds to spend during standard checkpoint operations on compacting
# smaller pages. Is should be less than the maximum checkpoint duration configured by
# "server.checkpoint.duration". If set to -1 checkpoints won't have a time limit. Regardless his
# value at least one page will be compacted for each checkpoint.
#server.checkpoint.compaction=

# Maximum dirty bytes percentage at which a pages will be considered for rebuild during a checkpoint.
# This value must be between 0 and 1.0.
#server.checkpoint.page.dirty.max.threshold=

# Minimum byte fill percentage at which a pages will be considered for rebuild during a checkpoint.
# This value must be between 0 and 1.0.
#server.checkpoint.page.fill.min.threshold=

# Maximum time (in ms) to consider a transaction as 'abandoned' and automatically rolled back, use 0 to disable
#server.abandoned.transactions.timeout=900000

# option to halt the JVM in case of error during the boot of a tablespace
# use this option to debug boot problems
server.halt.on.tablespace.boot.error=true

# users file, you'd better to set UNIX permissions properly
server.users.file=../conf/users

# overall limit on memory usage. it defaults to maximum heap size configured on the JVM
#server.memory.max.limit=

# maximum amount of memory (in bytes) used for data. Defaults to 30% of server.memory.max.limit
#server.memory.data.limit=

# maximum amount of memory (in bytes) used for primary indexes. Defaults to 20% of server.memory.max.limit
#server.memory.pk.limit=

# enable/disable JMX
#server.jmx.enable=true

# web ui
#http.enable=true

# bind address, default to same host of the main API connector
#http.host=

# http server TCP port
#http.port=9845


# Thread pools (if you find more sensible defaults please create and issue for discussion)

# size of internal Netty EventLoopGroups (boss and workers)
server.network.thread.workers=16

# threads for handling requests (mostly SQL parsing + dataccess/locks + reads)
server.network.thread.callback.workers=64

# threads for handling executions of activity which happen after writing to the log (mostly DML)
server.async.thread.workers=64
